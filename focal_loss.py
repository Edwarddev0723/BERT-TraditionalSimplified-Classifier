"""
Focal Loss å¯¦ç¾ - è™•ç†é¡åˆ¥ä¸å¹³è¡¡

åœ¨ classifier_finetune_v6_optimized.ipynb ä¸­ä½¿ç”¨æ­¤ä»£ç¢¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Loss - å°ˆæ³¨æ–¼å›°é›£æ¨£æœ¬
    
    è«–æ–‡: Focal Loss for Dense Object Detection
    https://arxiv.org/abs/1708.02002
    
    åƒæ•¸:
        alpha: é¡åˆ¥æ¬Šé‡ï¼Œç”¨æ–¼å¹³è¡¡æ­£è² æ¨£æœ¬
               - float: æ‰€æœ‰é¡åˆ¥ä½¿ç”¨ç›¸åŒæ¬Šé‡
               - list/tensor: æ¯å€‹é¡åˆ¥çš„æ¬Šé‡
        gamma: èª¿è£½å› å­ï¼Œæ§åˆ¶å°ç°¡å–®æ¨£æœ¬çš„é™æ¬Šç¨‹åº¦
               - gamma=0: ç­‰åŒæ–¼æ¨™æº–äº¤å‰ç†µ
               - gammaè¶Šå¤§: è¶Šé—œæ³¨å›°é›£æ¨£æœ¬
        reduction: 'none' | 'mean' | 'sum'
        label_smoothing: æ¨™ç±¤å¹³æ»‘åƒæ•¸ (0-1)
    
    ä½¿ç”¨ç¯„ä¾‹:
        # æ–¹æ³•1: è‡ªå‹•è¨ˆç®—é¡åˆ¥æ¬Šé‡
        loss_fct = FocalLoss(alpha='auto', gamma=2.0)
        
        # æ–¹æ³•2: æ‰‹å‹•æŒ‡å®šæ¬Šé‡
        loss_fct = FocalLoss(alpha=[0.4, 0.6], gamma=2.0)
        
        # è¨“ç·´æ™‚
        outputs = model(input_ids, attention_mask)
        loss = loss_fct(outputs.logits, labels)
    """
    
    def __init__(self, 
                 alpha=0.25, 
                 gamma=2.0, 
                 reduction='mean',
                 label_smoothing=0.0):
        super().__init__()
        
        # é¡åˆ¥æ¬Šé‡
        if isinstance(alpha, (list, tuple)):
            self.alpha = torch.tensor(alpha)
        elif isinstance(alpha, torch.Tensor):
            self.alpha = alpha
        elif alpha == 'auto':
            # å°‡åœ¨ç¬¬ä¸€æ¬¡forwardæ™‚è‡ªå‹•è¨ˆç®—
            self.alpha = None
        else:
            self.alpha = alpha
        
        self.gamma = gamma
        self.reduction = reduction
        self.label_smoothing = label_smoothing
        
        print(f"âœ“ FocalLoss initialized:")
        print(f"   alpha={alpha}, gamma={gamma}")
        print(f"   label_smoothing={label_smoothing}")
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: æ¨¡å‹è¼¸å‡º logits, shape: (batch_size, num_classes)
            targets: çœŸå¯¦æ¨™ç±¤, shape: (batch_size,)
        """
        # è¨ˆç®—äº¤å‰ç†µï¼ˆä¸é€²è¡Œreductionï¼‰
        ce_loss = F.cross_entropy(
            inputs, 
            targets, 
            reduction='none',
            label_smoothing=self.label_smoothing
        )
        
        # è¨ˆç®—æ¦‚ç‡
        pt = torch.exp(-ce_loss)  # pt: æ­£ç¢ºé¡åˆ¥çš„é æ¸¬æ¦‚ç‡
        
        # Focal term: (1 - pt)^gamma
        focal_term = (1 - pt) ** self.gamma
        
        # Focal loss
        focal_loss = focal_term * ce_loss
        
        # æ‡‰ç”¨é¡åˆ¥æ¬Šé‡
        if self.alpha is not None:
            if self.alpha == 'auto':
                # è‡ªå‹•è¨ˆç®—ï¼ˆç¬¬ä¸€æ¬¡èª¿ç”¨æ™‚ï¼‰
                # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²åœ¨åˆå§‹åŒ–æ™‚è¨ˆç®—
                pass
            else:
                if isinstance(self.alpha, torch.Tensor):
                    alpha_t = self.alpha.to(inputs.device)
                    # æ ¹æ“šç›®æ¨™é¸æ“‡å°æ‡‰çš„alpha
                    alpha_t = alpha_t[targets]
                else:
                    alpha_t = self.alpha
                
                focal_loss = alpha_t * focal_loss
        
        # Reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class WeightedFocalLoss(nn.Module):
    """
    åŠ æ¬Š Focal Loss - è‡ªå‹•æ ¹æ“šé¡åˆ¥åˆ†å¸ƒè¨ˆç®—æ¬Šé‡
    
    ä½¿ç”¨ç¯„ä¾‹:
        # åœ¨è¨“ç·´å‰è¨ˆç®—é¡åˆ¥æ¬Šé‡
        label_counts = train_df['label'].value_counts().sort_index()
        class_weights = len(train_df) / (len(label_counts) * label_counts.values)
        
        loss_fct = WeightedFocalLoss(
            class_weights=class_weights,
            gamma=2.0
        )
    """
    
    def __init__(self, class_weights=None, gamma=2.0, label_smoothing=0.05):
        super().__init__()
        
        if class_weights is not None:
            if not isinstance(class_weights, torch.Tensor):
                class_weights = torch.tensor(class_weights, dtype=torch.float32)
            self.register_buffer('class_weights', class_weights)
        else:
            self.class_weights = None
        
        self.gamma = gamma
        self.label_smoothing = label_smoothing
        
        print(f"âœ“ WeightedFocalLoss initialized:")
        print(f"   class_weights={class_weights}")
        print(f"   gamma={gamma}, label_smoothing={label_smoothing}")
    
    def forward(self, inputs, targets):
        # è¨ˆç®—äº¤å‰ç†µ
        ce_loss = F.cross_entropy(
            inputs, 
            targets, 
            weight=self.class_weights,
            reduction='none',
            label_smoothing=self.label_smoothing
        )
        
        # è¨ˆç®—æ¦‚ç‡
        pt = torch.exp(-ce_loss)
        
        # Focal term
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        
        return focal_loss.mean()


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

def example_usage_in_notebook():
    """
    åœ¨ classifier_finetune_v6_optimized.ipynb ä¸­çš„ä½¿ç”¨æ–¹å¼
    """
    
    # ===== æ–¹æ³•1: åœ¨æ¨¡å‹å®šç¾©ä¸­ä½¿ç”¨ =====
    
    # ä¿®æ”¹ OptimizedBertClassifier çš„ forward æ–¹æ³•
    """
    class OptimizedBertClassifier(nn.Module):
        def __init__(self, ...):
            super().__init__()
            # ... å…¶ä»–åˆå§‹åŒ–
            
            # æ·»åŠ  Focal Loss
            self.focal_loss = FocalLoss(alpha=0.4, gamma=2.0, label_smoothing=0.05)
        
        def forward(self, ..., labels=None, ...):
            # ... å‰å‘å‚³æ’­
            logits = self.classifier(pooled_output)
            
            # è¨ˆç®—æå¤±
            total_loss = None
            if labels is not None:
                main_loss = self.focal_loss(logits, labels)
                total_loss = main_loss
                
                # è¼”åŠ©ä»»å‹™æå¤±ï¼ˆå¦‚æœæœ‰ï¼‰
                if category_logits is not None and category_labels is not None:
                    category_loss = F.cross_entropy(category_logits, category_labels)
                    total_loss = main_loss + CATEGORY_LOSS_WEIGHT * category_loss
            
            return SequenceClassifierOutput(loss=total_loss, logits=logits, ...)
    """
    
    # ===== æ–¹æ³•2: ä½¿ç”¨è‡ªå®šç¾© Trainer =====
    
    """
    from transformers import Trainer
    
    class FocalLossTrainer(Trainer):
        def __init__(self, *args, focal_loss=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.focal_loss = focal_loss or FocalLoss(alpha=0.4, gamma=2.0)
        
        def compute_loss(self, model, inputs, return_outputs=False):
            labels = inputs.pop("labels")
            outputs = model(**inputs)
            logits = outputs.logits
            
            # ä½¿ç”¨ Focal Loss
            loss = self.focal_loss(logits, labels)
            
            return (loss, outputs) if return_outputs else loss
    
    # ä½¿ç”¨
    trainer = FocalLossTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        focal_loss=FocalLoss(alpha=0.4, gamma=2.0, label_smoothing=0.05)
    )
    """
    
    # ===== æ–¹æ³•3: è¨ˆç®—é¡åˆ¥æ¬Šé‡ =====
    
    """
    # åœ¨æ•¸æ“šæº–å‚™éšæ®µ
    label_counts = train_df['label'].value_counts().sort_index()
    total = len(train_df)
    num_classes = len(label_counts)
    
    # è¨ˆç®—æ¬Šé‡: weight[i] = total / (num_classes * count[i])
    class_weights = [total / (num_classes * count) for count in label_counts.values]
    
    print("é¡åˆ¥æ¬Šé‡:")
    for i, (count, weight) in enumerate(zip(label_counts.values, class_weights)):
        label_name = 'å¤§é™¸ç¹é«”' if i == 0 else 'å°ç£ç¹é«”'
        print(f"   {label_name}: count={count:,}, weight={weight:.3f}")
    
    # ä½¿ç”¨åŠ æ¬Š Focal Loss
    loss_fct = WeightedFocalLoss(
        class_weights=class_weights,
        gamma=2.0,
        label_smoothing=0.05
    )
    """
    
    pass


# ===== å®Œæ•´é›†æˆä»£ç¢¼ =====

FOCAL_LOSS_INTEGRATION_CODE = """
# ===== åœ¨ classifier_finetune_v6_optimized.ipynb ä¸­æ·»åŠ  =====

# 1. åœ¨å°å…¥éƒ¨åˆ†æ·»åŠ 
from focal_loss import FocalLoss, WeightedFocalLoss

# 2. åœ¨æ•¸æ“šåˆ†æå¾Œè¨ˆç®—é¡åˆ¥æ¬Šé‡
print("\\nğŸ“Š è¨ˆç®—é¡åˆ¥æ¬Šé‡...")
label_counts = train_df['label'].value_counts().sort_index()
total = len(train_df)
num_classes = len(label_counts)

class_weights = torch.tensor([
    total / (num_classes * count) for count in label_counts.values
], dtype=torch.float32)

print("é¡åˆ¥æ¬Šé‡:")
for i, (count, weight) in enumerate(zip(label_counts.values, class_weights.numpy())):
    label_name = 'å¤§é™¸ç¹é«”' if i == 0 else 'å°ç£ç¹é«”'
    print(f"   {label_name}: count={count:,}, weight={weight:.3f}")

# 3. ä¿®æ”¹æ¨¡å‹çš„æå¤±è¨ˆç®—
class OptimizedBertClassifier(nn.Module):
    def __init__(self, model_name, num_labels=2, num_categories=None, 
                 use_attention_pooling=True, use_multi_sample_dropout=True,
                 msd_num_samples=5, msd_dropout_rate=0.3,
                 class_weights=None, use_focal_loss=True):  # æ–°å¢åƒæ•¸
        super().__init__()
        
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç¢¼ ...
        
        # æå¤±å‡½æ•¸
        self.use_focal_loss = use_focal_loss
        if use_focal_loss:
            self.focal_loss = WeightedFocalLoss(
                class_weights=class_weights,
                gamma=2.0,
                label_smoothing=LABEL_SMOOTHING
            )
        else:
            self.loss_fct = nn.CrossEntropyLoss(
                weight=class_weights,
                label_smoothing=LABEL_SMOOTHING
            )
    
    def forward(self, input_ids, attention_mask=None, labels=None, 
                category_labels=None, chunk_count=None, sample_id=None, **kwargs):
        
        # ... å‰å‘å‚³æ’­ä»£ç¢¼ ...
        
        # è¨ˆç®—æå¤±
        total_loss = None
        if labels is not None:
            if self.use_focal_loss:
                main_loss = self.focal_loss(logits, labels.view(-1))
            else:
                main_loss = self.loss_fct(
                    logits.view(-1, self.num_labels), 
                    labels.view(-1)
                )
            
            total_loss = main_loss
            
            # è¼”åŠ©ä»»å‹™æå¤±
            if category_logits is not None and category_labels is not None:
                category_loss = F.cross_entropy(
                    category_logits.view(-1, self.num_categories),
                    category_labels.view(-1)
                )
                total_loss = main_loss + CATEGORY_LOSS_WEIGHT * category_loss
        
        return SequenceClassifierOutput(
            loss=total_loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

# 4. å‰µå»ºæ¨¡å‹æ™‚å‚³å…¥æ¬Šé‡
model = OptimizedBertClassifier(
    model_name=str(encoder_path),
    num_labels=2,
    num_categories=num_categories,
    use_attention_pooling=USE_ATTENTION_POOLING,
    use_multi_sample_dropout=MULTI_SAMPLE_DROPOUT,
    msd_num_samples=MSD_NUM_SAMPLES,
    msd_dropout_rate=MSD_DROPOUT_RATE,
    class_weights=class_weights,  # å‚³å…¥é¡åˆ¥æ¬Šé‡
    use_focal_loss=True  # å•Ÿç”¨ Focal Loss
)

print(f"âœ“ æ¨¡å‹å‰µå»ºæˆåŠŸï¼Œä½¿ç”¨ Focal Loss")
"""

if __name__ == '__main__':
    print("=" * 60)
    print("Focal Loss å¯¦ç¾")
    print("=" * 60)
    print("\né€™å€‹æ–‡ä»¶æä¾›äº†è™•ç†é¡åˆ¥ä¸å¹³è¡¡çš„ Focal Loss å¯¦ç¾")
    print("\nä½¿ç”¨æ–¹å¼:")
    print("1. å°‡æ­¤æ–‡ä»¶ä¿å­˜ç‚º focal_loss.py")
    print("2. åœ¨ classifier_finetune_v6_optimized.ipynb ä¸­å°å…¥")
    print("3. åƒè€ƒ FOCAL_LOSS_INTEGRATION_CODE é€²è¡Œé›†æˆ")
    print("\n" + "=" * 60)
    
    # ç°¡å–®æ¸¬è©¦
    print("\næ¸¬è©¦ Focal Loss:")
    loss_fct = FocalLoss(alpha=0.4, gamma=2.0, label_smoothing=0.05)
    
    # æ¨¡æ“¬æ•¸æ“š
    logits = torch.randn(8, 2)  # batch_size=8, num_classes=2
    labels = torch.randint(0, 2, (8,))
    
    loss = loss_fct(logits, labels)
    print(f"Loss value: {loss.item():.4f}")
    print("\nâœ“ Focal Loss æ¸¬è©¦é€šéï¼")
