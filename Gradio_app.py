"""
Gradio App for BERT Traditional Chinese Classifier
ç”¨æ–¼æ¸¬è©¦ç¹é«”ä¸­æ–‡åˆ†é¡æ¨¡å‹ï¼ˆå¤§é™¸ç¹é«” vs å°ç£ç¹é«”ï¼‰çš„äº’å‹•å¼ä»‹é¢
"""

import gradio as gr
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from collections import Counter

# ========== é…ç½® ==========
REPO_ID = "renhehuang/bert-traditional-chinese-classifier"
LABELS = {0: "å¤§é™¸ç¹é«”", 1: "å°ç£ç¹é«”"}
MAX_LEN, STRIDE = 384, 128

# ========== åˆå§‹åŒ–æ¨¡å‹ ==========
print("ğŸ”„ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer...")
device = (
    "mps" if torch.backends.mps.is_available()
    else ("cuda" if torch.cuda.is_available() else "cpu")
)

tokenizer = AutoTokenizer.from_pretrained(REPO_ID, cache_dir=".cache")
model = AutoModelForSequenceClassification.from_pretrained(REPO_ID, cache_dir=".cache")
model.to(device).eval()
print(f"âœ… æ¨¡å‹å·²è¼‰å…¥è‡³ {device}")

# ========== å·¥å…·å‡½æ•¸ ==========
def chunk_encode(text, max_len=MAX_LEN, stride=STRIDE):
    """é•·æ–‡æœ¬åˆ†å¡Šç·¨ç¢¼"""
    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False)["input_ids"]
    if len(ids) <= max_len - 2:
        enc = tokenizer(text, truncation=True, max_length=max_len,
                        return_attention_mask=True, return_tensors="pt")
        return [enc]
    enc = tokenizer(text, truncation=True, max_length=max_len, stride=stride,
                    return_overflowing_tokens=True, return_attention_mask=True,
                    return_tensors="pt")
    return [{"input_ids": enc["input_ids"][i:i+1],
             "attention_mask": enc["attention_mask"][i:i+1]}
            for i in range(len(enc["input_ids"]))]


@torch.inference_mode()
def predict_single(text: str):
    """å–®æ¬¡æ¨è«–"""
    if not text or not text.strip():
        return "âš ï¸ è«‹è¼¸å…¥æ–‡æœ¬", "", {}
    
    chunks = chunk_encode(text)
    probs_all = []
    for ch in chunks:
        logits = model(
            input_ids=ch["input_ids"].to(device),
            attention_mask=ch["attention_mask"].to(device)
        ).logits
        probs_all.append(F.softmax(logits, dim=-1).cpu())
    
    avg = torch.cat(probs_all, 0).mean(0)
    label_id = int(avg.argmax())
    confidence = float(avg[label_id])
    
    # æ ¼å¼åŒ–è¼¸å‡º
    result_text = f"ğŸ·ï¸ **{LABELS[label_id]}**"
    confidence_text = f"ğŸ“Š ä¿¡å¿ƒåº¦: **{confidence:.2%}**"
    probabilities = {
        "å¤§é™¸ç¹é«”": float(avg[0]),
        "å°ç£ç¹é«”": float(avg[1])
    }
    
    return result_text, confidence_text, probabilities


@torch.inference_mode()
def predict_voting(text: str, n_runs: int = 3):
    """å¤šæ¬¡æŠ•ç¥¨æ¨è«–ï¼ˆMC Dropoutï¼‰"""
    if not text or not text.strip():
        return "âš ï¸ è«‹è¼¸å…¥æ–‡æœ¬", "", {}, ""
    
    chunks = chunk_encode(text)
    prev_training = model.training
    run_prob_list = []
    
    try:
        model.train()  # å•Ÿç”¨ dropout
        for _ in range(n_runs):
            probs_all = []
            for ch in chunks:
                logits = model(
                    input_ids=ch["input_ids"].to(device),
                    attention_mask=ch["attention_mask"].to(device)
                ).logits
                probs_all.append(F.softmax(logits, dim=-1).cpu())
            run_prob_list.append(torch.cat(probs_all, 0).mean(0))
    finally:
        model.train() if prev_training else model.eval()
    
    probs_stack = torch.stack(run_prob_list, 0)
    per_run_ids = probs_stack.argmax(-1).tolist()
    vote_counts = Counter(per_run_ids)
    mean_probs = probs_stack.mean(0)
    
    voted_id = max(vote_counts.items(), key=lambda kv: (kv[1], mean_probs[kv[0]].item()))[0]
    confidence = float(mean_probs[voted_id])
    
    # æ ¼å¼åŒ–è¼¸å‡º
    result_text = f"ğŸ·ï¸ **{LABELS[voted_id]}**"
    confidence_text = f"ğŸ“Š å¹³å‡ä¿¡å¿ƒåº¦: **{confidence:.2%}**"
    probabilities = {
        "å¤§é™¸ç¹é«”": float(mean_probs[0]),
        "å°ç£ç¹é«”": float(mean_probs[1])
    }
    vote_info = f"ğŸ—³ï¸ æŠ•ç¥¨çµæœ: {vote_counts[voted_id]}/{n_runs} æ¬¡"
    
    return result_text, confidence_text, probabilities, vote_info


# ========== Gradio ä»‹é¢ ==========
examples = [
    ["é€™å€‹è»Ÿä»¶çš„ç•Œé¢è¨­è¨ˆå¾—å¾ˆå¥½ã€‚"],
    ["é€™å€‹è»Ÿé«”çš„ä»‹é¢è¨­è¨ˆå¾—å¾ˆå¥½ã€‚"],
    ["æˆ‘éœ€è¦ä¸‹è¼‰é€™å€‹ç¨‹åºåˆ°è¨ˆç®—æ©Ÿä¸Šã€‚"],
    ["æˆ‘éœ€è¦ä¸‹è¼‰é€™å€‹ç¨‹å¼åˆ°é›»è…¦ä¸Šã€‚"],
    ["è«‹æ‰“é–‹è¦–é »è§€çœ‹æ•™ç¨‹ã€‚"],
    ["è«‹æ‰“é–‹å½±ç‰‡è§€çœ‹æ•™å­¸ã€‚"],
]

with gr.Blocks(title="BERT ç¹é«”ä¸­æ–‡åˆ†é¡å™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ” BERT ç¹é«”ä¸­æ–‡åˆ†é¡å™¨
    
    å€åˆ†ã€Œå¤§é™¸ç¹é«”ã€èˆ‡ã€Œå°ç£ç¹é«”ã€çš„ BERT åˆ†é¡æ¨¡å‹
    
    - æ”¯æ´é•·æ–‡æœ¬è‡ªå‹•åˆ†å¡Šè™•ç†ï¼ˆmax_len=384ï¼‰
    - æä¾›å–®æ¬¡æ¨è«–èˆ‡å¤šæ¬¡æŠ•ç¥¨ï¼ˆMC Dropoutï¼‰æ¨¡å¼
    """)
    
    with gr.Tab("ğŸ“ å–®æ¬¡æ¨è«–"):
        with gr.Row():
            with gr.Column():
                input_single = gr.Textbox(
                    label="è¼¸å…¥æ–‡æœ¬",
                    placeholder="è«‹è¼¸å…¥ç¹é«”ä¸­æ–‡æ–‡æœ¬...",
                    lines=5
                )
                btn_single = gr.Button("ğŸš€ é–‹å§‹åˆ†é¡", variant="primary")
            
            with gr.Column():
                output_label_single = gr.Markdown(label="é æ¸¬çµæœ")
                output_conf_single = gr.Markdown(label="ä¿¡å¿ƒåº¦")
                output_probs_single = gr.Label(label="æ©Ÿç‡åˆ†å¸ƒ", num_top_classes=2)
        
        gr.Examples(
            examples=examples,
            inputs=input_single,
            label="ç¯„ä¾‹æ–‡æœ¬"
        )
        
        btn_single.click(
            fn=predict_single,
            inputs=input_single,
            outputs=[output_label_single, output_conf_single, output_probs_single]
        )
    
    with gr.Tab("ğŸ—³ï¸ æŠ•ç¥¨æ¨è«–ï¼ˆMC Dropoutï¼‰"):
        with gr.Row():
            with gr.Column():
                input_voting = gr.Textbox(
                    label="è¼¸å…¥æ–‡æœ¬",
                    placeholder="è«‹è¼¸å…¥ç¹é«”ä¸­æ–‡æ–‡æœ¬...",
                    lines=5
                )
                n_runs = gr.Slider(
                    minimum=3,
                    maximum=10,
                    value=3,
                    step=1,
                    label="æŠ•ç¥¨æ¬¡æ•¸",
                    info="æ¨è«–æ¬¡æ•¸è¶Šå¤šï¼Œçµæœè¶Šç©©å®šä½†é€Ÿåº¦è¼ƒæ…¢"
                )
                btn_voting = gr.Button("ğŸš€ é–‹å§‹æŠ•ç¥¨åˆ†é¡", variant="primary")
            
            with gr.Column():
                output_label_voting = gr.Markdown(label="é æ¸¬çµæœ")
                output_conf_voting = gr.Markdown(label="å¹³å‡ä¿¡å¿ƒåº¦")
                output_probs_voting = gr.Label(label="å¹³å‡æ©Ÿç‡åˆ†å¸ƒ", num_top_classes=2)
                output_vote_info = gr.Markdown(label="æŠ•ç¥¨çµ±è¨ˆ")
        
        gr.Examples(
            examples=examples,
            inputs=input_voting,
            label="ç¯„ä¾‹æ–‡æœ¬"
        )
        
        btn_voting.click(
            fn=predict_voting,
            inputs=[input_voting, n_runs],
            outputs=[output_label_voting, output_conf_voting, output_probs_voting, output_vote_info]
        )
    
    with gr.Tab("â„¹ï¸ é—œæ–¼æ¨¡å‹"):
        gr.Markdown("""
        ## æ¨¡å‹è³‡è¨Š
        
        - **æ¨¡å‹**: ckiplab/bert-base-chinese
        - **ä»»å‹™**: ç¹é«”ä¸­æ–‡æ–‡æœ¬åˆ†é¡ï¼ˆå¤§é™¸ç¹é«” vs å°ç£ç¹é«”ï¼‰
        - **æº–ç¢ºç‡**: 87.71%
        - **è¨“ç·´æ¨£æœ¬**: 156,824
        
        ## æ¨™ç±¤å®šç¾©
        
        - **å¤§é™¸ç¹é«”ï¼ˆä¸­åœ‹ç¹é«”ï¼‰**: ä½¿ç”¨ã€Œè½¯ä»¶ã€è§†é¢‘ã€ç¨‹åºã€è®¡ç®—æœºã€ç­‰è©å½™
        - **å°ç£ç¹é«”**: ä½¿ç”¨ã€Œè»Ÿé«”ã€å½±ç‰‡ã€ç¨‹å¼ã€é›»è…¦ã€ç­‰è©å½™
        
        ## åŠŸèƒ½ç‰¹è‰²
        
        - âœ… é•·æ–‡æœ¬è‡ªå‹•åˆ†å¡Šè™•ç†ï¼ˆ384 tokensï¼Œstride 128ï¼‰
        - âœ… Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡
        - âœ… Multi-Sample Dropout æå‡æ³›åŒ–
        - âœ… MC Dropout æŠ•ç¥¨æå‡ç©©å¥æ€§
        
        ## ä½¿ç”¨å»ºè­°
        
        - å°æ–¼é‡è¦æ±ºç­–ï¼Œå»ºè­°ä½¿ç”¨ã€ŒæŠ•ç¥¨æ¨è«–ã€æ¨¡å¼ä¸¦è¨­å®š 5-10 æ¬¡æŠ•ç¥¨
        - ä¿¡å¿ƒåº¦ â‰¥ 85% çš„é æ¸¬è¼ƒç‚ºå¯é 
        - æ··ç”¨è©å½™ã€å°ˆæ¥­è¡“èªæˆ–æ¥µçŸ­æ–‡æœ¬å¯èƒ½å½±éŸ¿æº–ç¢ºåº¦
        
        ---
        
        ğŸ“¦ **æ¨¡å‹å€‰åº«**: [renhehuang/bert-traditional-chinese-classifier](https://huggingface.co/renhehuang/bert-traditional-chinese-classifier)
        
        ğŸ“„ **æˆæ¬Š**: Apache 2.0
        """)

if __name__ == "__main__":
    demo.launch(
        share=False,
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    )
