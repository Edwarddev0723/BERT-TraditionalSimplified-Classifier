{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b1a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY =\"sk-b56c488f33b94df297a6314bd037b805\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc67fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å•Ÿå‹•ç°¡é«”è½‰ç¹é«”è½‰æ›ç³»çµ±...\n",
      "============================================================\n",
      "ğŸ“‚ è¼‰å…¥è³‡æ–™é›†:  LLM_Aug_102925.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ è®€å–è³‡æ–™é›†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.76it/s]\n",
      "ğŸ“¥ è®€å–è³‡æ–™é›†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸè¼‰å…¥è³‡æ–™é›†ï¼Œå…± 102925 ç­†è¨˜éŒ„\n",
      "ğŸ“‹ æ¬„ä½: ['text', 'total_score', 'mainland_terms', 'text_length', 'source_type', 'fragment_length', 'fragment_index', 'fragment_start', 'fragment_end']\n",
      "\n",
      "ğŸš€ é–‹å§‹éåŒæ­¥æ‰¹æ¬¡è½‰æ›\n",
      "ğŸ¤– ä½¿ç”¨æ¨¡å‹: gemma-3-27b-it\n",
      "ğŸ“¦ æ‰¹æ¬¡å¤§å°: 10 ç­†/æ‰¹æ¬¡\n",
      "â±ï¸  é ä¼°æ‰¹æ¬¡æ•¸: 10293\n",
      "============================================================\n",
      "ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š102925 ç­†\n",
      "ğŸ“‹ ç¸½æ‰¹æ¬¡æ•¸: 10293\n",
      "ğŸ“‹ å¾…è™•ç†è³‡æ–™: 102925 ç­†\n",
      "ğŸ“‹ å·²å®Œæˆè³‡æ–™: 0 ç­†\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 699/10293 | è™•ç†ä¸­:   7%|â–‹         | 6980/102925 [47:52<15:21:48,  1.73ç­†/s, å®Œæˆç‡=6.8%, å·²è½‰æ›=6980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: 502, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='https://labor-openwebui.dgx-coolify.apmic.ai/api/chat/completions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 1495/10293 | è™•ç†ä¸­:  15%|â–ˆâ–        | 14941/102925 [1:40:05<318:01:44, 13.01s/ç­†, å®Œæˆç‡=14.5%, å·²è½‰æ›=14940]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8645/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86441/102925 [9:56:22<858:02:51, 187.39s/ç­†, å®Œæˆç‡=84.0%, å·²è½‰æ›=86440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8653/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86521/102925 [11:14:18<684:48:27, 150.29s/ç­†, å®Œæˆç‡=84.1%, å·²è½‰æ›=86520]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8654/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86531/102925 [13:16:18<1479:05:57, 324.80s/ç­†, å®Œæˆç‡=84.1%, å·²è½‰æ›=86530]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8656/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86551/102925 [14:17:21<1223:28:41, 268.99s/ç­†, å®Œæˆç‡=84.1%, å·²è½‰æ›=86550]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n",
      "âš ï¸  API éŒ¯èª¤: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8658/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86571/102925 [14:35:15<744:57:55, 163.99s/ç­†, å®Œæˆç‡=84.1%, å·²è½‰æ›=86570] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8660/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86591/102925 [15:17:42<711:07:01, 156.73s/ç­†, å®Œæˆç‡=84.1%, å·²è½‰æ›=86590]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 8673/10293 | è™•ç†ä¸­:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 86721/102925 [15:31:41<115:15:34, 25.61s/ç­†, å®Œæˆç‡=84.3%, å·²è½‰æ›=86720] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n",
      "âš ï¸  API éŒ¯èª¤: [Errno 54] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ‰¹æ¬¡ 10293/10293 | è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102925/102925 [17:24:35<00:00,  1.64ç­†/s, å®Œæˆç‡=100.0%, å·²è½‰æ›=102925]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š è½‰æ›çµæœçµ±è¨ˆ:\n",
      "  ğŸ“ ç°¡é«”ä¸­æ–‡è³‡æ–™: 102925 ç­†\n",
      "  ğŸ“ ç¹é«”ä¸­æ–‡è³‡æ–™: 102925 ç­†\n",
      "  ğŸ“ˆ ç¸½è³‡æ–™é‡: 205850 ç­†\n",
      "\n",
      "ğŸ”„ è½‰æ›ç¯„ä¾‹:\n",
      "  1.\n",
      "     ç°¡é«”: å—ä¼¤èººåœ¨ç—…åºŠä¸Šçš„æ²™çƒ­å¤ä¸½Â·æ²™ä¾å¡”è¿™äº›å¤©æ¥ä¸€ç›´åœ¨å¯»æ‰¾é€å¥¹åˆ°åŒ»é™¢çš„å‡ºç§Ÿè½¦é©¾é©¶å‘˜ï¼Œå¥¹è¯´...\n",
      "     ç¹é«”: å—å‚·èººåœ¨ç—…åºŠä¸Šçš„æ²™ç†±å¤éº—Â·æ²™ä¾å¡”é€™äº›å¤©ä¾†ä¸€ç›´åœ¨å°‹æ‰¾é€å¥¹åˆ°é†«é™¢çš„è¨ˆç¨‹è»Šé§•é§›å“¡ï¼Œå¥¹èªª...\n",
      "  2.\n",
      "     ç°¡é«”: è¿™ç»„æ•°æ®ç»™æˆ‘ä»¬è€å¸ˆæäº†ä¸ªé†’ï¼Œä»¥åå¯¹äºå’±è‡ªå·±çš„å¥åº·çœŸå¾—é•¿ç‚¹å„¿å¿ƒï¼Œä¸èƒ½å†å¿½è§†å¤§æ„äº†ã€‚\n",
      "     ç¹é«”: é€™çµ„æ•¸æ“šçµ¦æˆ‘å€‘è€å¸«æå€‹é†’ï¼Œä»¥å¾Œå°æ–¼è‡ªå·±çš„å¥åº·çœŸå¾—é•·é»å¿ƒï¼Œä¸èƒ½å†å¿½è¦–å¤§æ„äº†ã€‚\n",
      "  3.\n",
      "     ç°¡é«”: è¶…çº§åˆ’ç®—åšå·¥å¾ˆå¥½å¡çº¸å¾ˆåšçœ‹ç€å¾ˆæœ‰æ¡£æ¬¡çš„å…·ä½“æœ‰å¤šå°‘å¼ æˆ‘æ²¡æ•°ä¸è¿‡æŒºå¤šçš„ä½†ä¸æ˜¯ä¹¦çš„å½¢å¼...\n",
      "     ç¹é«”: è¶…ç´šåˆ’ç®—åšå·¥å¾ˆå¥½å¡ç´™å¾ˆåšçœ‹èµ·ä¾†å¾ˆæœ‰æª”æ¬¡çš„å…·é«”æœ‰å¤šå°‘å¼µæˆ‘æ²’æ•¸ä¸éæŒºå¤šçš„ä½†ä¸æ˜¯æ›¸çš„å½¢...\n",
      "\n",
      "ğŸ’¾ æ­£åœ¨å„²å­˜æª”æ¡ˆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“ å„²å­˜ç°¡é«”ä¸­æ–‡è³‡æ–™: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.78it/s]\n",
      "\n",
      "ğŸ“ å„²å­˜ç¹é«”ä¸­æ–‡è³‡æ–™: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.61it/s]\n",
      "ğŸ“ å„²å­˜ç¹é«”ä¸­æ–‡è³‡æ–™: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.61it/s]\n",
      "ğŸ“ å„²å­˜åˆä½µè³‡æ–™é›†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å„²å­˜å®Œæˆ:\n",
      "  ğŸ“„ ç°¡é«”ä¸­æ–‡è³‡æ–™: simplified_chinese_data_20251029_111151.csv (102925 ç­†)\n",
      "  ğŸ“„ ç¹é«”ä¸­æ–‡è³‡æ–™: traditional_chinese_data_20251029_111151.csv (102925 ç­†)\n",
      "  ğŸ“„ åˆä½µè³‡æ–™é›†: combined_chinese_data_20251029_111151.csv (205850 ç­†)\n",
      "  ğŸ“‹ JSONæ ¼å¼ä¹Ÿå·²å„²å­˜\n",
      "\n",
      "ğŸ§¹ è‡¨æ™‚æª”æ¡ˆä¿ç•™ç‹€æ…‹:\n",
      "   âœ“ conversion_checkpoint.json - é€²åº¦æª¢æŸ¥é»\n",
      "   âœ“ simplified_partial.csv - ç°¡é«”éƒ¨åˆ†çµæœ\n",
      "   âœ“ traditional_partial.csv - ç¹é«”éƒ¨åˆ†çµæœ\n",
      "   ğŸ’¡ å¦‚éœ€é‡æ–°é–‹å§‹ï¼Œè«‹æ‰‹å‹•åˆªé™¤é€™äº›æª”æ¡ˆ\n",
      "\n",
      "ğŸ‰ ç°¡é«”è½‰ç¹é«”è½‰æ›å®Œæˆï¼\n",
      "ğŸ“‹ å¯ç”¨è®Šæ•¸: simplified_chinese_data, traditional_chinese_data, combined_chinese_data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "CHECKPOINT_FILE = \"conversion_checkpoint.json\"\n",
    "PARTIAL_SIMPLIFIED_FILE = \"simplified_partial.csv\"\n",
    "PARTIAL_TRADITIONAL_FILE = \"traditional_partial.csv\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"è¼‰å…¥è™•ç†é€²åº¦\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            ckpt = json.load(f)\n",
    "        print(f\"ğŸ”„ åµæ¸¬åˆ°é€²åº¦æª”ï¼Œå°‡å¾ index {ckpt['last_index'] + 1} ç¹¼çºŒ\")\n",
    "        return ckpt[\"last_index\"], ckpt.get(\"processed_indices\", [])\n",
    "    return -1, []\n",
    "\n",
    "def save_checkpoint(last_index, processed_indices):\n",
    "    \"\"\"å„²å­˜è™•ç†é€²åº¦\"\"\"\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"last_index\": last_index,\n",
    "            \"processed_indices\": processed_indices\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# ğŸ¯ ç°¡é«”è½‰ç¹é«”ç³»çµ± - ä½¿ç”¨ API é€²è¡Œè½‰æ›\n",
    "print(\"ğŸš€ å•Ÿå‹•ç°¡é«”è½‰ç¹é«”è½‰æ›ç³»çµ±...\")\n",
    "\n",
    "# å®šç¾©å¤§é™¸ç”¨èªåˆ°å°ç£ç”¨èªçš„å°ç…§è¡¨\n",
    "mainland_to_taiwan_terms = {\n",
    "    \"è¨ˆç®—æ©Ÿ\": \"é›»è…¦\", \"è»Ÿä»¶\": \"è»Ÿé«”\", \"ç¡¬ä»¶\": \"ç¡¬é«”\", \"ç¶²çµ¡\": \"ç¶²è·¯\", \n",
    "    \"æ•¸æ“š\": \"è³‡æ–™\", \"ç¨‹åº\": \"ç¨‹å¼\", \"ä¿¡æ¯\": \"è³‡è¨Š\", \"å‡ºç§Ÿè»Š\": \"è¨ˆç¨‹è»Š\",\n",
    "    \"å…¬äº¤è»Š\": \"å…¬è»Š\", \"åœ°éµ\": \"æ·é‹\", \"è³ªé‡\": \"å“è³ª\", \"æœå‹™å“¡\": \"æœå‹™ç”Ÿ\",\n",
    "    \"åœŸè±†\": \"é¦¬éˆ´è–¯\", \"è¥¿ç´…æŸ¿\": \"ç•ªèŒ„\", \"é¼ æ¨™\": \"æ»‘é¼ \", \"æ‰“å°æ©Ÿ\": \"å°è¡¨æ©Ÿ\",\n",
    "    \"æ–‡ä»¶å¤¾\": \"è³‡æ–™å¤¾\", \"åšå®¢\": \"éƒ¨è½æ ¼\", \"è¦–é »\": \"å½±ç‰‡\", \"éŸ³é »\": \"éŸ³è¨Š\",\n",
    "    \"æ¿€æ´»\": \"å•Ÿç”¨\", \"é»˜èª\": \"é è¨­\", \"è¨»å†Š\": \"è¨»å†Š\", \"ç™»éŒ„\": \"ç™»å…¥\"\n",
    "}\n",
    "\n",
    "async def convert_to_traditional_api_async(text, session, model_endpoint, api_key, model_name):\n",
    "    \"\"\"ä½¿ç”¨ API éåŒæ­¥å°‡ç°¡é«”å¤§é™¸ç”¨èªè½‰æ›æˆå°ç£ç¹é«”ç”¨èª\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"è«‹å°‡ä»¥ä¸‹ç°¡é«”ä¸­æ–‡æ–‡æœ¬è½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. å°‡æ‰€æœ‰ç°¡é«”å­—è½‰æ›ç‚ºç¹é«”å­—\n",
    "2. å°‡å¤§é™¸ç”¨èªè½‰æ›ç‚ºå°ç£å¸¸ç”¨è©å½™ï¼ˆä¾‹å¦‚ï¼šè¨ˆç®—æ©Ÿâ†’é›»è…¦ã€è»Ÿä»¶â†’è»Ÿé«”ã€ç¶²çµ¡â†’ç¶²è·¯ç­‰ï¼‰\n",
    "3. ä¿æŒå¥å­çµæ§‹å’Œèªæ„ä¸è®Š\n",
    "4. åªå›å‚³è½‰æ›å¾Œçš„ç¹é«”ä¸­æ–‡æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–èªªæ˜\n",
    "\n",
    "åŸæ–‡ï¼š{text}\n",
    "\n",
    "ç¹é«”ä¸­æ–‡ï¼š\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with session.post(model_endpoint, headers=headers, json=payload, timeout=90) as response:\n",
    "            data = await response.json()\n",
    "            reply = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", None)\n",
    "            \n",
    "            # æ¸…ç†å›æ‡‰ï¼Œç§»é™¤å¯èƒ½çš„å‰ç¶´èªªæ˜\n",
    "            if reply:\n",
    "                reply = reply.strip()\n",
    "                # ç§»é™¤å¯èƒ½çš„å‰ç¶´å¦‚ \"ç¹é«”ä¸­æ–‡ï¼š\" ç­‰\n",
    "                prefixes = [\"ç¹é«”ä¸­æ–‡ï¼š\", \"ç¹é«”ç‰ˆæœ¬ï¼š\", \"è½‰æ›çµæœï¼š\", \"å°ç£ç¹é«”ï¼š\"]\n",
    "                for prefix in prefixes:\n",
    "                    if reply.startswith(prefix):\n",
    "                        reply = reply[len(prefix):].strip()\n",
    "            \n",
    "            return reply if reply else text  # å¦‚æœè½‰æ›å¤±æ•—ï¼Œè¿”å›åŸæ–‡\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  API éŒ¯èª¤: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def basic_simplified_to_traditional(text):\n",
    "    \"\"\"åŸºæœ¬çš„ç°¡ç¹è½‰æ›ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰\"\"\"\n",
    "    # ä½¿ç”¨è©å½™å°ç…§è¡¨é€²è¡ŒåŸºæœ¬è½‰æ›\n",
    "    result = text\n",
    "    for simp, trad in mainland_to_taiwan_terms.items():\n",
    "        result = result.replace(simp, trad)\n",
    "    return result\n",
    "\n",
    "\n",
    "async def process_dataset_async_batched(df, model_endpoint, api_key, model_name=\"gemma-3-27b-it\",\n",
    "                                        text_col='text', batch_size=10):\n",
    "    \"\"\"\n",
    "    éåŒæ­¥æ‰¹æ¬¡è™•ç†è³‡æ–™é›†ï¼Œå°‡ç°¡é«”è½‰ç¹é«”\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š è™•ç†è³‡æ–™é›†ï¼š{len(df)} ç­†\")\n",
    "    \n",
    "    last_index, processed_indices = load_checkpoint()\n",
    "    processed_set = set(processed_indices)\n",
    "    \n",
    "    simplified_results = []\n",
    "    traditional_results = []\n",
    "\n",
    "    # è¼‰å…¥å·²è™•ç†çš„éƒ¨åˆ†çµæœ\n",
    "    if os.path.exists(PARTIAL_SIMPLIFIED_FILE):\n",
    "        simp_df = pd.read_csv(PARTIAL_SIMPLIFIED_FILE)\n",
    "        simplified_results = simp_df.to_dict(\"records\")\n",
    "        print(f\"ğŸ” å·²è®€å–ç°¡é«”è³‡æ–™çµæœï¼š{len(simplified_results)} ç­†\")\n",
    "    \n",
    "    if os.path.exists(PARTIAL_TRADITIONAL_FILE):\n",
    "        trad_df = pd.read_csv(PARTIAL_TRADITIONAL_FILE)\n",
    "        traditional_results = trad_df.to_dict(\"records\")\n",
    "        print(f\"ğŸ” å·²è®€å–ç¹é«”è³‡æ–™çµæœï¼š{len(traditional_results)} ç­†\")\n",
    "\n",
    "    texts = df[text_col].tolist()\n",
    "    indices = df.index.tolist()\n",
    "    \n",
    "    # è¨ˆç®—ç¸½æ‰¹æ¬¡æ•¸å’Œéœ€è¦è™•ç†çš„è³‡æ–™æ•¸\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    total_to_process = len([idx for idx in indices if idx not in processed_set])\n",
    "    \n",
    "    print(f\"ğŸ“‹ ç¸½æ‰¹æ¬¡æ•¸: {total_batches}\")\n",
    "    print(f\"ğŸ“‹ å¾…è™•ç†è³‡æ–™: {total_to_process} ç­†\")\n",
    "    print(f\"ğŸ“‹ å·²å®Œæˆè³‡æ–™: {len(processed_set)} ç­†\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # ä½¿ç”¨ tqdm å‰µå»ºä¸»é€²åº¦æ¢\n",
    "        with tqdm(total=total_to_process, desc=\"ğŸ”„ ç¸½é«”è½‰æ›é€²åº¦\", unit=\"ç­†\") as pbar:\n",
    "            for batch_start in range(0, len(texts), batch_size):\n",
    "                batch_texts = texts[batch_start:batch_start+batch_size]\n",
    "                batch_indices = indices[batch_start:batch_start+batch_size]\n",
    "\n",
    "                # è·³éå·²è™•ç†çš„ç´¢å¼•\n",
    "                filtered_batch = [\n",
    "                    (idx, text) for idx, text in zip(batch_indices, batch_texts)\n",
    "                    if idx not in processed_set\n",
    "                ]\n",
    "                \n",
    "                if not filtered_batch:\n",
    "                    continue\n",
    "\n",
    "                filtered_indices, filtered_texts = zip(*filtered_batch) if filtered_batch else ([], [])\n",
    "                \n",
    "                # é¡¯ç¤ºç•¶å‰æ‰¹æ¬¡è³‡è¨Š\n",
    "                batch_num = (batch_start // batch_size) + 1\n",
    "                pbar.set_description(f\"ğŸ”„ æ‰¹æ¬¡ {batch_num}/{total_batches} | è™•ç†ä¸­\")\n",
    "\n",
    "                # åŸ·è¡Œ API è½‰æ›\n",
    "                tasks = [\n",
    "                    convert_to_traditional_api_async(text, session, model_endpoint, api_key, model_name)\n",
    "                    for text in filtered_texts\n",
    "                ]\n",
    "                responses = await asyncio.gather(*tasks)\n",
    "\n",
    "                for i, response in enumerate(responses):\n",
    "                    idx = filtered_indices[i]\n",
    "                    original_text = filtered_texts[i]\n",
    "                    traditional_text = response if response else basic_simplified_to_traditional(original_text)\n",
    "                    \n",
    "                    # ç²å–åŸå§‹è³‡æ–™çš„å…¶ä»–æ¬„ä½\n",
    "                    original_row = df.iloc[idx]\n",
    "                    \n",
    "                    # ç°¡é«”è³‡æ–™ï¼ˆåŸå§‹è³‡æ–™ï¼‰\n",
    "                    simp_record = {\n",
    "                        'text': original_text,\n",
    "                        'text_length': len(original_text),\n",
    "                        'language_type': 'ç°¡é«”ä¸­æ–‡'\n",
    "                    }\n",
    "                    \n",
    "                    # ç¹é«”è³‡æ–™ï¼ˆè½‰æ›å¾Œï¼‰\n",
    "                    trad_record = {\n",
    "                        'text': traditional_text,\n",
    "                        'text_length': len(traditional_text),\n",
    "                        'language_type': 'ç¹é«”ä¸­æ–‡',\n",
    "                        'original_text': original_text  # ä¿ç•™åŸæ–‡ä»¥ä¾¿å°ç…§\n",
    "                    }\n",
    "                    \n",
    "                    # è¤‡è£½å…¶ä»–æ¬„ä½\n",
    "                    for col in df.columns:\n",
    "                        if col != text_col and col in original_row:\n",
    "                            simp_record[col] = original_row[col]\n",
    "                            trad_record[col] = original_row[col]\n",
    "                    \n",
    "                    simplified_results.append(simp_record)\n",
    "                    traditional_results.append(trad_record)\n",
    "                    processed_set.add(idx)\n",
    "                    \n",
    "                    # æ›´æ–°é€²åº¦æ¢\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # å®šæœŸä¿å­˜é€²åº¦\n",
    "                if len(filtered_batch) > 0:\n",
    "                    pd.DataFrame(simplified_results).to_csv(PARTIAL_SIMPLIFIED_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "                    pd.DataFrame(traditional_results).to_csv(PARTIAL_TRADITIONAL_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "                    save_checkpoint(max(processed_set), list(processed_set))\n",
    "                    \n",
    "                    # æ›´æ–°é€²åº¦æ¢çš„å¾Œç¶´è³‡è¨Š\n",
    "                    completion_rate = (len(processed_set) / len(df)) * 100\n",
    "                    pbar.set_postfix({\n",
    "                        'å®Œæˆç‡': f'{completion_rate:.1f}%',\n",
    "                        'å·²è½‰æ›': len(processed_set)\n",
    "                    })\n",
    "\n",
    "    return simplified_results, traditional_results\n",
    "\n",
    "\n",
    "def save_final_results(simplified_results, traditional_results):\n",
    "    \"\"\"å„²å­˜æœ€çµ‚çµæœ\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"\\nğŸ’¾ æ­£åœ¨å„²å­˜æª”æ¡ˆ...\")\n",
    "    \n",
    "    # å„²å­˜ç°¡é«”ä¸­æ–‡è³‡æ–™\n",
    "    simp_df = pd.DataFrame(simplified_results)\n",
    "    simp_csv = f\"simplified_chinese_data_{timestamp}.csv\"\n",
    "    simp_json = f\"simplified_chinese_data_{timestamp}.json\"\n",
    "    \n",
    "    with tqdm(total=2, desc=\"ğŸ“ å„²å­˜ç°¡é«”ä¸­æ–‡è³‡æ–™\") as pbar:\n",
    "        simp_df.to_csv(simp_csv, index=False, encoding='utf-8-sig')\n",
    "        pbar.update(1)\n",
    "        simp_df.to_json(simp_json, orient='records', force_ascii=False, indent=2)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # å„²å­˜ç¹é«”ä¸­æ–‡è³‡æ–™\n",
    "    trad_df = pd.DataFrame(traditional_results)\n",
    "    trad_csv = f\"traditional_chinese_data_{timestamp}.csv\"\n",
    "    trad_json = f\"traditional_chinese_data_{timestamp}.json\"\n",
    "    \n",
    "    with tqdm(total=2, desc=\"ğŸ“ å„²å­˜ç¹é«”ä¸­æ–‡è³‡æ–™\") as pbar:\n",
    "        trad_df.to_csv(trad_csv, index=False, encoding='utf-8-sig')\n",
    "        pbar.update(1)\n",
    "        trad_df.to_json(trad_json, orient='records', force_ascii=False, indent=2)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # å„²å­˜åˆä½µè³‡æ–™é›†\n",
    "    combined_df = pd.concat([simp_df, trad_df], ignore_index=True)\n",
    "    combined_csv = f\"combined_chinese_data_{timestamp}.csv\"\n",
    "    \n",
    "    with tqdm(total=1, desc=\"ğŸ“ å„²å­˜åˆä½µè³‡æ–™é›†\") as pbar:\n",
    "        combined_df.to_csv(combined_csv, index=False, encoding='utf-8-sig')\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nâœ… å„²å­˜å®Œæˆ:\")\n",
    "    print(f\"  ğŸ“„ ç°¡é«”ä¸­æ–‡è³‡æ–™: {simp_csv} ({len(simp_df)} ç­†)\")\n",
    "    print(f\"  ğŸ“„ ç¹é«”ä¸­æ–‡è³‡æ–™: {trad_csv} ({len(trad_df)} ç­†)\")\n",
    "    print(f\"  ğŸ“„ åˆä½µè³‡æ–™é›†: {combined_csv} ({len(combined_df)} ç­†)\")\n",
    "    print(f\"  ğŸ“‹ JSONæ ¼å¼ä¹Ÿå·²å„²å­˜\")\n",
    "    \n",
    "    return simp_df, trad_df, combined_df\n",
    "\n",
    "\n",
    "# ä¸»è¦åŸ·è¡Œæµç¨‹\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è¼‰å…¥ç›®æ¨™è³‡æ–™é›†\n",
    "data_path = \" LLM_Aug_102925.csv\"  # æ³¨æ„æª”åå‰æœ‰ç©ºæ ¼\n",
    "print(f\"ğŸ“‚ è¼‰å…¥è³‡æ–™é›†: {data_path}\")\n",
    "\n",
    "try:\n",
    "    with tqdm(total=1, desc=\"ğŸ“¥ è®€å–è³‡æ–™é›†\") as pbar:\n",
    "        source_df = pd.read_csv(data_path)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™é›†ï¼Œå…± {len(source_df)} ç­†è¨˜éŒ„\")\n",
    "    print(f\"ğŸ“‹ æ¬„ä½: {source_df.columns.tolist()}\")\n",
    "    \n",
    "    # è¨­å®š API åƒæ•¸\n",
    "    MODEL_API_ENDPOINT = \"https://labor-openwebui.dgx-coolify.apmic.ai/api/chat/completions\"\n",
    "    OPENWEBUI_API_KEY = API_KEY\n",
    "    MODEL_NAME = \"gemma-3-27b-it\"\n",
    "    BATCH_SIZE = 10  # æ¯æ‰¹è™•ç†æ•¸é‡\n",
    "    \n",
    "    print(f\"\\nğŸš€ é–‹å§‹éåŒæ­¥æ‰¹æ¬¡è½‰æ›\")\n",
    "    print(f\"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}\")\n",
    "    print(f\"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} ç­†/æ‰¹æ¬¡\")\n",
    "    print(f\"â±ï¸  é ä¼°æ‰¹æ¬¡æ•¸: {(len(source_df) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # åŸ·è¡Œè½‰æ›\n",
    "    simplified_results, traditional_results = await process_dataset_async_batched(\n",
    "        df=source_df,\n",
    "        model_endpoint=MODEL_API_ENDPOINT,\n",
    "        api_key=OPENWEBUI_API_KEY,\n",
    "        model_name=MODEL_NAME,\n",
    "        text_col='text',\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # çµ±è¨ˆçµæœ\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“Š è½‰æ›çµæœçµ±è¨ˆ:\")\n",
    "    print(f\"  ğŸ“ ç°¡é«”ä¸­æ–‡è³‡æ–™: {len(simplified_results)} ç­†\")\n",
    "    print(f\"  ğŸ“ ç¹é«”ä¸­æ–‡è³‡æ–™: {len(traditional_results)} ç­†\")\n",
    "    print(f\"  ğŸ“ˆ ç¸½è³‡æ–™é‡: {len(simplified_results) + len(traditional_results)} ç­†\")\n",
    "    \n",
    "    # é¡¯ç¤ºè½‰æ›ç¯„ä¾‹\n",
    "    if traditional_results:\n",
    "        print(f\"\\nğŸ”„ è½‰æ›ç¯„ä¾‹:\")\n",
    "        for i in range(min(3, len(traditional_results))):\n",
    "            simp_preview = simplified_results[i]['text'][:40] + \"...\" if len(simplified_results[i]['text']) > 40 else simplified_results[i]['text']\n",
    "            trad_preview = traditional_results[i]['text'][:40] + \"...\" if len(traditional_results[i]['text']) > 40 else traditional_results[i]['text']\n",
    "            print(f\"  {i+1}.\")\n",
    "            print(f\"     ç°¡é«”: {simp_preview}\")\n",
    "            print(f\"     ç¹é«”: {trad_preview}\")\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    simp_df, trad_df, combined_df = save_final_results(simplified_results, traditional_results)\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['simplified_chinese_data'] = simp_df\n",
    "    globals()['traditional_chinese_data'] = trad_df\n",
    "    globals()['combined_chinese_data'] = combined_df\n",
    "    \n",
    "    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆï¼ˆå¯é¸ï¼‰\n",
    "    print(f\"\\nğŸ§¹ è‡¨æ™‚æª”æ¡ˆä¿ç•™ç‹€æ…‹:\")\n",
    "    print(f\"   âœ“ {CHECKPOINT_FILE} - é€²åº¦æª¢æŸ¥é»\")\n",
    "    print(f\"   âœ“ {PARTIAL_SIMPLIFIED_FILE} - ç°¡é«”éƒ¨åˆ†çµæœ\")\n",
    "    print(f\"   âœ“ {PARTIAL_TRADITIONAL_FILE} - ç¹é«”éƒ¨åˆ†çµæœ\")\n",
    "    print(f\"   ğŸ’¡ å¦‚éœ€é‡æ–°é–‹å§‹ï¼Œè«‹æ‰‹å‹•åˆªé™¤é€™äº›æª”æ¡ˆ\")\n",
    "    # os.remove(CHECKPOINT_FILE)\n",
    "    # os.remove(PARTIAL_SIMPLIFIED_FILE)\n",
    "    # os.remove(PARTIAL_TRADITIONAL_FILE)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ç°¡é«”è½‰ç¹é«”è½‰æ›å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“‹ å¯ç”¨è®Šæ•¸: simplified_chinese_data, traditional_chinese_data, combined_chinese_data\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ æ‰¾ä¸åˆ°è³‡æ–™é›†: {data_path}\")\n",
    "    print(f\"ğŸ’¡ è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢º\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb284d",
   "metadata": {},
   "source": [
    "# ğŸ”„ å­—å½¢çµ±ä¸€è™•ç†\n",
    "\n",
    "ç‚ºäº†é˜²æ­¢æ¨¡å‹ã€Œä½œå¼Šã€ï¼ˆåƒ…æ ¹æ“šç°¡ç¹å­—é«”å·®ç•°åˆ†é¡ï¼‰ï¼Œæˆ‘å€‘éœ€è¦ï¼š\n",
    "1. **ç°¡é«”è³‡æ–™**ï¼šå­—å½¢è½‰ç‚ºç¹é«”ï¼Œä½†è©å½™ä¿æŒå¤§é™¸ç”¨èªï¼ˆå¦‚ã€Œè¨ˆç®—æ©Ÿã€ï¼‰\n",
    "2. **ç¹é«”è³‡æ–™**ï¼šå·²ç¶“æ˜¯å°ç£ç¹é«”ç”¨èªï¼ˆå¦‚ã€Œé›»è…¦ã€ï¼‰\n",
    "\n",
    "é€™æ¨£æ¨¡å‹å¿…é ˆå­¸ç¿’**è©å½™å’Œèªæ³•å·®ç•°**ï¼Œè€Œéå­—å½¢å·®ç•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56594d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å­—å½¢è½‰æ›æ¸¬è©¦:\n",
      "  ç°¡é«”: æˆ‘åœ¨ç”¨è®¡ç®—æœºå¤„ç†æ•°æ®\n",
      "  ç¹é«”: æˆ‘åœ¨ç”¨è¨ˆç®—æ©Ÿè™•ç†æ•¸æ“š\n",
      "\n",
      "  ç°¡é«”: è¿™ä¸ªè½¯ä»¶å¾ˆå¥½ç”¨\n",
      "  ç¹é«”: é€™å€‹è»Ÿä»¶å¾ˆå¥½ç”¨\n",
      "\n",
      "  ç°¡é«”: ç½‘ç»œè¿æ¥å¾ˆå¿«\n",
      "  ç¹é«”: ç¶²çµ¡é€£æ¥å¾ˆå¿«\n",
      "\n",
      "  ç°¡é«”: å‡ºç§Ÿè½¦å¸æœºå¾ˆå‹å¥½\n",
      "  ç¹é«”: å‡ºç§Ÿè»Šå¸æ©Ÿå¾ˆå‹å¥½\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# å­—å½¢çµ±ä¸€è½‰æ›å‡½æ•¸\n",
    "def convert_simplified_to_traditional_form_only(text):\n",
    "    \"\"\"\n",
    "    åƒ…å°‡ç°¡é«”å­—å½¢è½‰æ›ç‚ºç¹é«”å­—å½¢ï¼Œä¸æ”¹è®Šè©å½™\n",
    "    ä¾‹å¦‚ï¼š\"è®¡ç®—æœº\" -> \"è¨ˆç®—æ©Ÿ\"ï¼ˆè€Œé \"é›»è…¦\"ï¼‰\n",
    "    ä½¿ç”¨ OpenCC çš„ s2tï¼ˆç°¡é«”åˆ°ç¹é«”ï¼‰è½‰æ›\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from opencc import OpenCC\n",
    "        cc = OpenCC('s2t')  # ç°¡é«”åˆ°ç¹é«”ï¼ˆåƒ…å­—å½¢ï¼‰\n",
    "        return cc.convert(text)\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  OpenCC æœªå®‰è£ï¼Œä½¿ç”¨åŸºæœ¬è½‰æ›\")\n",
    "        # å‚™ç”¨æ–¹æ¡ˆï¼šè¿”å›åŸæ–‡\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  è½‰æ›éŒ¯èª¤: {e}\")\n",
    "        return text\n",
    "\n",
    "# æ¸¬è©¦å­—å½¢è½‰æ›\n",
    "print(\"ğŸ“ å­—å½¢è½‰æ›æ¸¬è©¦:\")\n",
    "test_cases = [\n",
    "    \"æˆ‘åœ¨ç”¨è®¡ç®—æœºå¤„ç†æ•°æ®\",\n",
    "    \"è¿™ä¸ªè½¯ä»¶å¾ˆå¥½ç”¨\",\n",
    "    \"ç½‘ç»œè¿æ¥å¾ˆå¿«\",\n",
    "    \"å‡ºç§Ÿè½¦å¸æœºå¾ˆå‹å¥½\"\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    converted = convert_simplified_to_traditional_form_only(text)\n",
    "    print(f\"  ç°¡é«”: {text}\")\n",
    "    print(f\"  ç¹é«”: {converted}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea614d",
   "metadata": {},
   "source": [
    "# ğŸ“Š æº–å‚™å°æ¯”å­¸ç¿’è³‡æ–™é›†\n",
    "\n",
    "å°‡è½‰æ›å¾Œçš„è³‡æ–™æº–å‚™æˆå–®å¥å°æ¯”å­¸ç¿’æ ¼å¼ï¼Œé©ç”¨æ–¼ `contractive_finetune.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5c0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é–‹å§‹æº–å‚™å°æ¯”å­¸ç¿’è³‡æ–™é›†...\n",
      "============================================================\n",
      "ğŸ¯ æº–å‚™å°æ¯”å­¸ç¿’è³‡æ–™é›†ï¼ˆå–®å¥æ ¼å¼ï¼‰\n",
      "ğŸ”„ å•Ÿç”¨å­—å½¢çµ±ä¸€ï¼šå°‡ç°¡é«”å­—è½‰ç‚ºç¹é«”å­—ï¼Œä½†ä¿æŒè©å½™å·®ç•°\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å°‡ç°¡é«”è³‡æ–™çš„å­—å½¢çµ±ä¸€ç‚ºç¹é«”...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å­—å½¢è½‰æ›: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102925/102925 [00:03<00:00, 33540.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å­—å½¢çµ±ä¸€å®Œæˆ\n",
      "\n",
      "ğŸ“ å­—å½¢çµ±ä¸€ç¯„ä¾‹:\n",
      "  1.\n",
      "     åŸå§‹ç°¡é«”: å—ä¼¤èººåœ¨ç—…åºŠä¸Šçš„æ²™çƒ­å¤ä¸½Â·æ²™ä¾å¡”è¿™äº›å¤©æ¥ä¸€ç›´åœ¨å¯»æ‰¾é€å¥¹åˆ°åŒ»é™¢çš„å‡ºç§Ÿè½¦é©¾é©¶å‘˜ï¼Œå¥¹è¯´ï¼šâ€œè¿™ä¸ªæ±‰æ—å°ä¼™å­æ˜¯æˆ‘çš„æ•‘å‘½æ©äººï¼Œä»–åœ¨æˆ‘...\n",
      "     ç¹é«”å­—å½¢: å—å‚·èººåœ¨ç—…ç‰€ä¸Šçš„æ²™ç†±å¤éº—Â·æ²™ä¾å¡”é€™äº›å¤©ä¾†ä¸€ç›´åœ¨å°‹æ‰¾é€å¥¹åˆ°é†«é™¢çš„å‡ºç§Ÿè»Šé§•é§›å“¡ï¼Œå¥¹èªªï¼šâ€œé€™å€‹æ¼¢æ—å°å¤¥å­æ˜¯æˆ‘çš„æ•‘å‘½æ©äººï¼Œä»–åœ¨æˆ‘...\n",
      "\n",
      "  2.\n",
      "     åŸå§‹ç°¡é«”: è¿™ç»„æ•°æ®ç»™æˆ‘ä»¬è€å¸ˆæäº†ä¸ªé†’ï¼Œä»¥åå¯¹äºå’±è‡ªå·±çš„å¥åº·çœŸå¾—é•¿ç‚¹å„¿å¿ƒï¼Œä¸èƒ½å†å¿½è§†å¤§æ„äº†ã€‚...\n",
      "     ç¹é«”å­—å½¢: é€™çµ„æ•¸æ“šçµ¦æˆ‘å€‘è€å¸«æäº†å€‹é†’ï¼Œä»¥å¾Œå°æ–¼å’±è‡ªå·±çš„å¥åº·çœŸå¾—é•·é»å…’å¿ƒï¼Œä¸èƒ½å†å¿½è¦–å¤§æ„äº†ã€‚...\n",
      "\n",
      "  3.\n",
      "     åŸå§‹ç°¡é«”: è¶…çº§åˆ’ç®—åšå·¥å¾ˆå¥½å¡çº¸å¾ˆåšçœ‹ç€å¾ˆæœ‰æ¡£æ¬¡çš„å…·ä½“æœ‰å¤šå°‘å¼ æˆ‘æ²¡æ•°ä¸è¿‡æŒºå¤šçš„ä½†ä¸æ˜¯ä¹¦çš„å½¢å¼æ¯ä¸ªæ‰‹å·¥éƒ½æ˜¯å•ç‹¬çš„ä¸€å¼ å¡çº¸é¢œè‰²éå¸¸å¥½æ²¡æœ‰...\n",
      "     ç¹é«”å­—å½¢: è¶…ç´šåˆ’ç®—åšå·¥å¾ˆå¥½å¡ç´™å¾ˆåšçœ‹ç€å¾ˆæœ‰æª”æ¬¡çš„å…·é«”æœ‰å¤šå°‘å¼µæˆ‘æ²’æ•¸ä¸éæŒºå¤šçš„ä½†ä¸æ˜¯æ›¸çš„å½¢å¼æ¯å€‹æ‰‹å·¥éƒ½æ˜¯å–®ç¨çš„ä¸€å¼µå¡ç´™é¡è‰²éå¸¸å¥½æ²’æœ‰...\n",
      "\n",
      "\n",
      "ğŸ“Š è³‡æ–™é›†çµ±è¨ˆ:\n",
      "  ç°¡é«”ä¸­æ–‡ (Label 0 - å¤§é™¸ç”¨èª): 102,925 ç­†\n",
      "  ç¹é«”ä¸­æ–‡ (Label 1 - å°ç£ç”¨èª): 102,925 ç­†\n",
      "  ç¸½è¨ˆ: 205,850 ç­†\n",
      "\n",
      "âœ… å­—å½¢å·²çµ±ä¸€ç‚ºç¹é«”ï¼Œæ¨¡å‹éœ€å­¸ç¿’è©å½™å’Œèªæ³•å·®ç•°\n",
      "   ä¾‹å¦‚: è¨ˆç®—æ©Ÿ (å¤§é™¸) vs é›»è…¦ (å°ç£)\n",
      "\n",
      "ğŸ“‹ æ¨™ç±¤åˆ†å¸ƒ:\n",
      "label\n",
      "0    102925\n",
      "1    102925\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ§¹ æ¸…ç†è³‡æ–™...\n",
      "  ç§»é™¤ç©ºç™½/é‡è¤‡å¾Œ: 196,031 ç­† (ç§»é™¤ 9,819 ç­†)\n",
      "\n",
      "âœ‚ï¸  åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†...\n",
      "\n",
      "ğŸ“Š åˆ†å‰²çµæœ:\n",
      "  è¨“ç·´é›†: 176,427 ç­†\n",
      "  é©—è­‰é›†: 19,604 ç­†\n",
      "\n",
      "ğŸ“‹ è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ:\n",
      "label\n",
      "0    90778\n",
      "1    85649\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“‹ é©—è­‰é›†æ¨™ç±¤åˆ†å¸ƒ:\n",
      "label\n",
      "0    10087\n",
      "1     9517\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ’¾ å„²å­˜è³‡æ–™é›†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å„²å­˜é€²åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å„²å­˜å®Œæˆ:\n",
      "  ğŸ“„ è¨“ç·´é›†: data_cache_v4/train_contrastive.parquet\n",
      "  ğŸ“„ é©—è­‰é›†: data_cache_v4/val_contrastive.parquet\n",
      "  ğŸ“„ å®Œæ•´è³‡æ–™: data_cache_v4/cleaned_dataset_contrastive.parquet\n",
      "\n",
      "ğŸ¯ å°æ¯”å­¸ç¿’è³‡æ–™é›†æº–å‚™å®Œæˆï¼ˆå­—å½¢å·²çµ±ä¸€ï¼‰ï¼\n",
      "   æ¨¡å‹å°‡å­¸ç¿’: è©å½™å·®ç•° + èªæ³•å·®ç•°ï¼Œè€Œéå­—å½¢å·®ç•°\n",
      "ğŸ“ å¯ç›´æ¥ç”¨æ–¼ contractive_finetune.ipynb è¨“ç·´\n",
      "============================================================\n",
      "\n",
      "âœ… è³‡æ–™å·²æº–å‚™å®Œæˆï¼å¯ä»¥é–‹å§‹å°æ¯”å­¸ç¿’è¨“ç·´\n",
      "ğŸ“‹ å¯ç”¨è®Šæ•¸: contrastive_train_df, contrastive_val_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_contrastive_learning_data(simplified_df, traditional_df, output_dir=\"data_cache_v4\", \n",
    "                                      unify_character_form=True):\n",
    "    \"\"\"\n",
    "    æº–å‚™å–®å¥å°æ¯”å­¸ç¿’è³‡æ–™é›†\n",
    "    åƒè€ƒ contractive_finetune.ipynb ç¬¬ 1582-1607 è¡Œçš„å¯¦ä½œ\n",
    "    \n",
    "    Args:\n",
    "        simplified_df: ç°¡é«”ä¸­æ–‡è³‡æ–™ DataFrame\n",
    "        traditional_df: ç¹é«”ä¸­æ–‡è³‡æ–™ DataFrame\n",
    "        output_dir: è¼¸å‡ºç›®éŒ„\n",
    "        unify_character_form: æ˜¯å¦çµ±ä¸€å­—å½¢ï¼ˆå°‡ç°¡é«”å­—è½‰ç‚ºç¹é«”å­—ï¼Œä½†ä¿æŒè©å½™ä¸è®Šï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df: è¨“ç·´é›†å’Œé©—è­‰é›†\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ¯ æº–å‚™å°æ¯”å­¸ç¿’è³‡æ–™é›†ï¼ˆå–®å¥æ ¼å¼ï¼‰\")\n",
    "    if unify_character_form:\n",
    "        print(\"ğŸ”„ å•Ÿç”¨å­—å½¢çµ±ä¸€ï¼šå°‡ç°¡é«”å­—è½‰ç‚ºç¹é«”å­—ï¼Œä½†ä¿æŒè©å½™å·®ç•°\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. ç¢ºä¿è³‡æ–™æœ‰æ­£ç¢ºçš„æ¨™ç±¤\n",
    "    simp_df_copy = simplified_df.copy()\n",
    "    trad_df_copy = traditional_df.copy()\n",
    "    \n",
    "    # 2. å­—å½¢çµ±ä¸€è™•ç†ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰\n",
    "    if unify_character_form:\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨å°‡ç°¡é«”è³‡æ–™çš„å­—å½¢çµ±ä¸€ç‚ºç¹é«”...\")\n",
    "        from opencc import OpenCC\n",
    "        cc = OpenCC('s2t')  # ç°¡é«”åˆ°ç¹é«”ï¼ˆåƒ…å­—å½¢è½‰æ›ï¼‰\n",
    "        \n",
    "        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦\n",
    "        tqdm.pandas(desc=\"å­—å½¢è½‰æ›\")\n",
    "        simp_df_copy['text'] = simp_df_copy['text'].progress_apply(\n",
    "            lambda x: cc.convert(str(x)) if pd.notna(x) else x\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… å­—å½¢çµ±ä¸€å®Œæˆ\")\n",
    "        \n",
    "        # é¡¯ç¤ºè½‰æ›ç¯„ä¾‹\n",
    "        print(f\"\\nğŸ“ å­—å½¢çµ±ä¸€ç¯„ä¾‹:\")\n",
    "        for i in range(min(3, len(simplified_df))):\n",
    "            original = simplified_df.iloc[i]['text'][:60]\n",
    "            converted = simp_df_copy.iloc[i]['text'][:60]\n",
    "            print(f\"  {i+1}.\")\n",
    "            print(f\"     åŸå§‹ç°¡é«”: {original}...\")\n",
    "            print(f\"     ç¹é«”å­—å½¢: {converted}...\")\n",
    "            print()\n",
    "    \n",
    "    # 3. è¨­å®šæ¨™ç±¤ï¼š0 = ç°¡é«”ï¼ˆå¤§é™¸ç”¨èªï¼‰ï¼Œ1 = ç¹é«”ï¼ˆå°ç£ç”¨èªï¼‰\n",
    "    simp_df_copy['label'] = 0\n",
    "    trad_df_copy['label'] = 1\n",
    "    \n",
    "    # 4. åˆä½µè³‡æ–™é›†\n",
    "    clean_df = pd.concat([simp_df_copy, trad_df_copy], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š è³‡æ–™é›†çµ±è¨ˆ:\")\n",
    "    print(f\"  ç°¡é«”ä¸­æ–‡ (Label 0 - å¤§é™¸ç”¨èª): {len(simp_df_copy):,} ç­†\")\n",
    "    print(f\"  ç¹é«”ä¸­æ–‡ (Label 1 - å°ç£ç”¨èª): {len(trad_df_copy):,} ç­†\")\n",
    "    print(f\"  ç¸½è¨ˆ: {len(clean_df):,} ç­†\")\n",
    "    \n",
    "    if unify_character_form:\n",
    "        print(f\"\\nâœ… å­—å½¢å·²çµ±ä¸€ç‚ºç¹é«”ï¼Œæ¨¡å‹éœ€å­¸ç¿’è©å½™å’Œèªæ³•å·®ç•°\")\n",
    "        print(f\"   ä¾‹å¦‚: è¨ˆç®—æ©Ÿ (å¤§é™¸) vs é›»è…¦ (å°ç£)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "    print(clean_df['label'].value_counts().sort_index())\n",
    "    \n",
    "    # 5. æª¢æŸ¥å¿…è¦æ¬„ä½\n",
    "    required_cols = ['text', 'label']\n",
    "    missing_cols = [col for col in required_cols if col not in clean_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}\")\n",
    "    \n",
    "    # 6. æ¸…ç†è³‡æ–™\n",
    "    print(f\"\\nğŸ§¹ æ¸…ç†è³‡æ–™...\")\n",
    "    initial_count = len(clean_df)\n",
    "    \n",
    "    # ç§»é™¤ç©ºç™½æ–‡æœ¬\n",
    "    clean_df = clean_df[clean_df['text'].notna()]\n",
    "    clean_df = clean_df[clean_df['text'].str.strip() != '']\n",
    "    \n",
    "    # ç§»é™¤é‡è¤‡æ–‡æœ¬ï¼ˆä¿ç•™ç¬¬ä¸€å€‹ï¼‰\n",
    "    clean_df = clean_df.drop_duplicates(subset=['text'], keep='first')\n",
    "    \n",
    "    print(f\"  ç§»é™¤ç©ºç™½/é‡è¤‡å¾Œ: {len(clean_df):,} ç­† (ç§»é™¤ {initial_count - len(clean_df):,} ç­†)\")\n",
    "    \n",
    "    # 7. åˆ†å±¤åˆ†å‰²ï¼ˆstratified splitï¼‰\n",
    "    print(f\"\\nâœ‚ï¸  åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        clean_df, \n",
    "        test_size=0.1, \n",
    "        random_state=42,\n",
    "        stratify=clean_df['label']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åˆ†å‰²çµæœ:\")\n",
    "    print(f\"  è¨“ç·´é›†: {len(train_df):,} ç­†\")\n",
    "    print(f\"  é©—è­‰é›†: {len(val_df):,} ç­†\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "    print(train_df['label'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ é©—è­‰é›†æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "    print(val_df['label'].value_counts().sort_index())\n",
    "    \n",
    "    # 8. å„²å­˜è³‡æ–™é›†\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_path = Path(output_dir) / \"train_contrastive.parquet\"\n",
    "    val_path = Path(output_dir) / \"val_contrastive.parquet\"\n",
    "    full_path = Path(output_dir) / \"cleaned_dataset_contrastive.parquet\"\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ å„²å­˜è³‡æ–™é›†...\")\n",
    "    with tqdm(total=3, desc=\"å„²å­˜é€²åº¦\") as pbar:\n",
    "        train_df.to_parquet(train_path, index=False, compression='zstd')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        val_df.to_parquet(val_path, index=False, compression='zstd')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        clean_df.to_parquet(full_path, index=False, compression='zstd')\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nâœ… å„²å­˜å®Œæˆ:\")\n",
    "    print(f\"  ğŸ“„ è¨“ç·´é›†: {train_path}\")\n",
    "    print(f\"  ğŸ“„ é©—è­‰é›†: {val_path}\")\n",
    "    print(f\"  ğŸ“„ å®Œæ•´è³‡æ–™: {full_path}\")\n",
    "    \n",
    "    if unify_character_form:\n",
    "        print(f\"\\nğŸ¯ å°æ¯”å­¸ç¿’è³‡æ–™é›†æº–å‚™å®Œæˆï¼ˆå­—å½¢å·²çµ±ä¸€ï¼‰ï¼\")\n",
    "        print(f\"   æ¨¡å‹å°‡å­¸ç¿’: è©å½™å·®ç•° + èªæ³•å·®ç•°ï¼Œè€Œéå­—å½¢å·®ç•°\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ¯ å°æ¯”å­¸ç¿’è³‡æ–™é›†æº–å‚™å®Œæˆï¼\")\n",
    "    \n",
    "    print(f\"ğŸ“ å¯ç›´æ¥ç”¨æ–¼ contractive_finetune.ipynb è¨“ç·´\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "# åŸ·è¡Œè³‡æ–™æº–å‚™\n",
    "if 'simplified_chinese_data' in globals() and 'traditional_chinese_data' in globals():\n",
    "    print(\"\\né–‹å§‹æº–å‚™å°æ¯”å­¸ç¿’è³‡æ–™é›†...\")\n",
    "    \n",
    "    # å•Ÿç”¨å­—å½¢çµ±ä¸€\n",
    "    contrastive_train_df, contrastive_val_df = prepare_contrastive_learning_data(\n",
    "        simplified_chinese_data,\n",
    "        traditional_chinese_data,\n",
    "        output_dir=\"data_cache_v4\",\n",
    "        unify_character_form=True  # ğŸ”‘ é—œéµåƒæ•¸ï¼šçµ±ä¸€å­—å½¢\n",
    "    )\n",
    "    \n",
    "    # è¨­å®šå…¨åŸŸè®Šæ•¸\n",
    "    globals()['contrastive_train_df'] = contrastive_train_df\n",
    "    globals()['contrastive_val_df'] = contrastive_val_df\n",
    "    \n",
    "    print(\"\\nâœ… è³‡æ–™å·²æº–å‚™å®Œæˆï¼å¯ä»¥é–‹å§‹å°æ¯”å­¸ç¿’è¨“ç·´\")\n",
    "    print(\"ğŸ“‹ å¯ç”¨è®Šæ•¸: contrastive_train_df, contrastive_val_df\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  è«‹å…ˆåŸ·è¡Œä¸Šä¸€å€‹ cell ç”Ÿæˆ simplified_chinese_data å’Œ traditional_chinese_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181b2bd",
   "metadata": {},
   "source": [
    "# ğŸš€ å¦‚ä½•ä½¿ç”¨å°æ¯”å­¸ç¿’è¨“ç·´\n",
    "\n",
    "## ğŸ”‘ é—œéµæ”¹é€²ï¼šå­—å½¢çµ±ä¸€\n",
    "\n",
    "ç‚ºäº†é˜²æ­¢æ¨¡å‹ã€Œä½œå¼Šã€ï¼ˆåƒ…æ ¹æ“šç°¡ç¹å­—é«”åˆ¤æ–·ï¼‰ï¼Œæˆ‘å€‘æ¡ç”¨**å­—å½¢çµ±ä¸€**ç­–ç•¥ï¼š\n",
    "\n",
    "### è³‡æ–™è™•ç†æ–¹å¼\n",
    "\n",
    "1. **ç°¡é«”è³‡æ–™ï¼ˆLabel 0 - å¤§é™¸ç”¨èªï¼‰**\n",
    "   - âœ… å­—å½¢ï¼šç°¡é«” â†’ ç¹é«”ï¼ˆå¦‚ï¼šè®¡ç®—æœº â†’ è¨ˆç®—æ©Ÿï¼‰\n",
    "   - âœ… è©å½™ï¼šä¿æŒå¤§é™¸ç”¨èªï¼ˆå¦‚ï¼šè¨ˆç®—æ©Ÿã€è»Ÿä»¶ã€ç¶²çµ¡ï¼‰\n",
    "   \n",
    "2. **ç¹é«”è³‡æ–™ï¼ˆLabel 1 - å°ç£ç”¨èªï¼‰**\n",
    "   - âœ… å­—å½¢ï¼šç¹é«”ï¼ˆä¿æŒä¸è®Šï¼‰\n",
    "   - âœ… è©å½™ï¼šå°ç£ç”¨èªï¼ˆå¦‚ï¼šé›»è…¦ã€è»Ÿé«”ã€ç¶²è·¯ï¼‰\n",
    "\n",
    "### å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "æ¨¡å‹**ç„¡æ³•**ä¾è³´å­—å½¢å·®ç•°ï¼Œå¿…é ˆå­¸ç¿’ï¼š\n",
    "- âœ… **è©å½™å·®ç•°**ï¼šè¨ˆç®—æ©Ÿ vs é›»è…¦ã€è»Ÿä»¶ vs è»Ÿé«”\n",
    "- âœ… **èªæ³•ç¿’æ…£**ï¼šæŒºå¥½çš„ vs å¾ˆå¥½ã€å’‹æ¨£ vs æ€éº¼æ¨£\n",
    "- âœ… **è¡¨é”æ–¹å¼**ï¼šå¤§é™¸å£èª vs å°ç£å£èª\n",
    "\n",
    "### ç¯„ä¾‹å°æ¯”\n",
    "\n",
    "```\n",
    "ç°¡é«”è³‡æ–™ï¼ˆå­—å½¢å·²è½‰ç¹é«”ï¼Œè©å½™ä¿æŒå¤§é™¸ç”¨èªï¼‰ï¼š\n",
    "\"é€™å€‹è¨ˆç®—æ©Ÿçš„è»Ÿä»¶é‹è¡Œå¾—æŒºå¿«çš„\"\n",
    "      â†‘         â†‘        â†‘\n",
    "   å¤§é™¸ç”¨èª   å¤§é™¸ç”¨èª   å¤§é™¸èªæ³•\n",
    "\n",
    "ç¹é«”è³‡æ–™ï¼ˆå°ç£ç”¨èªï¼‰ï¼š\n",
    "\"é€™å€‹é›»è…¦çš„è»Ÿé«”é‹è¡Œå¾—å¾ˆå¿«\"\n",
    "      â†‘      â†‘      â†‘\n",
    "   å°ç£ç”¨èª  å°ç£ç”¨èª  å°ç£èªæ³•\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## è³‡æ–™æ ¼å¼èªªæ˜\n",
    "\n",
    "æº–å‚™å¥½çš„è³‡æ–™é›†åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š\n",
    "- `text`: ä¸­æ–‡æ–‡æœ¬ï¼ˆ**å­—å½¢çµ±ä¸€ç‚ºç¹é«”**ï¼‰\n",
    "- `label`: æ¨™ç±¤ (0=å¤§é™¸ç”¨èª, 1=å°ç£ç”¨èª)\n",
    "- å…¶ä»–åŸå§‹æ¬„ä½ï¼ˆå¦‚ `text_length`, `language_type` ç­‰ï¼‰\n",
    "\n",
    "## è¨“ç·´æ­¥é©Ÿ\n",
    "\n",
    "### 1. ä½¿ç”¨ç¾æœ‰çš„ contractive_finetune.ipynb\n",
    "\n",
    "è³‡æ–™å·²å„²å­˜åœ¨ `data_cache_v4/` ç›®éŒ„ï¼š\n",
    "- `train_contrastive.parquet` - è¨“ç·´é›†\n",
    "- `val_contrastive.parquet` - é©—è­‰é›†\n",
    "- `cleaned_dataset_contrastive.parquet` - å®Œæ•´è³‡æ–™é›†\n",
    "\n",
    "ç›´æ¥åœ¨ `contractive_finetune.ipynb` ä¸­ä¿®æ”¹è³‡æ–™è·¯å¾‘ï¼š\n",
    "\n",
    "```python\n",
    "# è¼‰å…¥è³‡æ–™ï¼ˆå­—å½¢å·²çµ±ä¸€ï¼‰\n",
    "clean_df = pd.read_parquet(\"data_cache_v4/cleaned_dataset_contrastive.parquet\")\n",
    "```\n",
    "\n",
    "### 2. å°æ¯”å­¸ç¿’çš„å„ªå‹¢\n",
    "\n",
    "- âœ… **å­¸ç¿’çœŸå¯¦å·®ç•°**ï¼šå°ˆæ³¨æ–¼è©å½™å’Œèªæ³•ï¼Œè€Œéå­—å½¢\n",
    "- âœ… **æ›´å¼·çš„æ³›åŒ–èƒ½åŠ›**ï¼šä¸æœƒè¢«å­—å½¢æ··æ·†\n",
    "- âœ… **æ›´å¥½çš„è¡¨å¾µç©ºé–“**ï¼šåŒé¡ç”¨èªåœ¨å‘é‡ç©ºé–“ä¸­æ›´æ¥è¿‘\n",
    "- âœ… **å¯¦éš›æ‡‰ç”¨åƒ¹å€¼**ï¼šèƒ½å€åˆ†çœŸå¯¦çš„èªè¨€ä½¿ç”¨å·®ç•°\n",
    "\n",
    "### 3. è¨“ç·´é…ç½®å»ºè­°\n",
    "\n",
    "åƒè€ƒ `contractive_finetune.ipynb` çš„è¨­å®šï¼š\n",
    "\n",
    "```python\n",
    "# æ¨¡å‹\n",
    "BASE_MODEL = \"ckiplab/bert-base-chinese\"\n",
    "\n",
    "# è¨“ç·´åƒæ•¸\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "TEMPERATURE = 0.07  # å°æ¯”å­¸ç¿’æº«åº¦åƒæ•¸\n",
    "\n",
    "# è³‡æ–™å¢å¼·\n",
    "MASK_PROB = 0.12\n",
    "MAX_MASK_TOKENS = 3\n",
    "```\n",
    "\n",
    "### 4. é æœŸæ•ˆæœ\n",
    "\n",
    "ä½¿ç”¨å­—å½¢çµ±ä¸€å¾Œï¼Œæ¨¡å‹æ‡‰è©²èƒ½å¤ ï¼š\n",
    "1. âœ… æ­£ç¢ºè­˜åˆ¥å¤§é™¸ç”¨èªå’Œå°ç£ç”¨èªçš„å·®ç•°\n",
    "2. âœ… ç†è§£èªæ³•ç¿’æ…£çš„ä¸åŒ\n",
    "3. âœ… åœ¨æ··åˆå­—å½¢çš„æ–‡æœ¬ä¸­ä¹Ÿèƒ½æº–ç¢ºåˆ†é¡\n",
    "4. âœ… å…·æœ‰æ›´å¼·çš„å¯¦éš›æ‡‰ç”¨åƒ¹å€¼\n",
    "\n",
    "### 5. è©•ä¼°æŒ‡æ¨™\n",
    "\n",
    "- **æº–ç¢ºç‡ (Accuracy)**ï¼šæ•´é«”åˆ†é¡æ­£ç¢ºç‡\n",
    "- **F1-Score**ï¼šå¹³è¡¡ç²¾ç¢ºç‡å’Œå¬å›ç‡\n",
    "- **æ··æ·†çŸ©é™£**ï¼šæŸ¥çœ‹èª¤åˆ†é¡æƒ…æ³\n",
    "- **å°æ¯”æå¤± (Contrastive Loss)**ï¼šè¨“ç·´éç¨‹ä¸­çš„ä¸»è¦æå¤±\n",
    "- **ç›¸ä¼¼åº¦é–“éš” (Similarity Gap)**ï¼šåŒé¡ vs ç•°é¡çš„ç›¸ä¼¼åº¦å·®è·\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€æ­¥**ï¼šåŸ·è¡Œ `contractive_finetune.ipynb` é–‹å§‹è¨“ç·´ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
